"""
ä¸€æ™‚è¨˜æ†¶ã®å®Ÿè£…ã‚¯ãƒ©ã‚¹ã§ã™ã€‚
å®šç¾©
    ä¸€æ™‚çš„ã«è¨˜æ†¶ã‚’è¦ç´ ã¨ã—ã¦ãã‚Œã‚’ä¿æŒã—ã¦ãŠãè¦ç´ æ•° ğ›¼ ã®æœ‰é™é›†åˆã€‚ğ‘Šã®è¦ç´ æ•°ãŒ ğ›¼ ã®æ™‚ã«è¦ç´ ã‚’è¿½åŠ ã™ã‚‹å ´
    åˆã€ğ‘Š ã®ä¸­ã®è¦ç´ ã®ã©ã‚Œã‹ãŒãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã°ã‚Œä¸Šæ›¸ãã•ã‚Œã‚‹ã€‚è¦ç´ æ•°ğ‘˜ | ğ‘˜ â‰¤ ğ›¼ã®ğ‘Šã¨è¨˜æ†¶ã‚’è¦ç´ ã¨ã™ã‚‹è¦ç´ æ•°
    ğ‘™ | ğ‘™ â‰¤ ğ›¼ å€‹ã®é›†åˆã¨çµåˆã™ã‚‹å ´åˆã€ğ‘Š ã®ä¸­ã‹ã‚‰ ğ‘˜ + ğ‘™ âˆ’ ğ›¼ å€‹ã®è¦ç´ ãŒãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã°ã‚Œä¸Šæ›¸ãã•ã‚Œã‚‹ã€‚

The Implementation class of "Working Memory".
*Definition
    Working Memory is a finite set with a maximum number of elements Î± that holds memories temporarily as an
    element. If the elements are added when the number of elements in W is Î±, the elements in W are randomly chosen
    and overwritten
"""
import torch
import numpy as np
from typing import *
from logger import *

setLogger(__name__,DEBUG)

class WorkingMemory:
    """
    ã“ã®ã‚¯ãƒ©ã‚¹ã¯æ¬¡ã®ã‚ˆã†ã«ã€è¨˜æ†¶ã‚’æ ¼ç´ã—ã¦ãŠããŸã‚ã®ã‚¯ãƒ©ã‚¹ã§ã™ã€‚
        W = {M_1, M_2, M_3}
    """

    def __init__(self, max_length:int, id_type:np.dtype=np.int64) -> None:
        """
        ç©ºã®Working Memory ã‚’ä½œæˆã—ã¾ã™
        Create a empty Working Memory.
        """

        self.id_type = id_type
        type_info = np.iinfo(id_type)
        self.min_id = 0
        self.max_id = type_info.max
        self.max_length = max_length

        self.memories = np.empty((0,),dtype=id_type)

    def add(self, input_memories:Union[int,list[int], np.ndarray,torch.Tensor], 
            is_sorted=True, is_duplicated=False) -> None:
        """
        Working Memoryã«è¨˜æ†¶ã‚’è¿½åŠ ã—ã¾ã™ã€‚
        ã™ã§ã«Working Memory ãŒã„ã£ã±ã„ã®ã¨ãã¯ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã°ã‚Œã€ä¸Šæ›¸ãã•ã‚Œã¾ã™ã€‚
        input_memoriesãŒæ˜‡é †ã«sortã„ãªã„å ´åˆã¯is_sorted=Falseã«ã—ã¦ãã ã•ã„ã€‚
        input_memoriesã®ä¸­ã«ã«é‡è¤‡ã—ãŸè¨˜æ†¶IDãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯is_duplicated=Trueã«ã—ã¦ãã ã•ã„

        Add input_memories to Working Memory.
        If the elements are added when the number of elements in W is maximum, 
        the elements in W are randomly chosen and overwritten.
        If input_memories is not sorted in ascending order, please set is_sorted=False
        If there are duplicate memory IDs in input_memories, please set is_duplicated=True
        """
        
        input_memories = self._type_check(input_memories)

        # fix duplication and sort.
        if is_duplicated:
            input_memories = np.unique(input_memories)
        elif not is_sorted:
            input_memories = np.sort(input_memories)

        # concatenation
        idxes = np.searchsorted(input_memories,self.memories)
        n_exist = np.logical_not(input_memories[idxes] == input_memories)
        mem = self.memories[n_exist]
        np.random.shuffle(mem)
        cut_len = self.max_length - len(input_memories)
        self.memories = np.concatenate([input_memories,mem[:cut_len]],axis=0)

    def _type_check(self,input_memories) -> np.ndarray:
        t = type(input_memories)
        if t is list:
            input_memories = np.array(input_memories,self.id_type)
        elif t is int:
            input_memories = np.array([t],self.id_type)
        elif t is torch.Tensor:
            input_memories = input_memories.detach().cpu().numpy()
        else:
            raise ValueError("Unknow id data type! {}".format(t))
        
        input_memories = input_memories.astype(self.id_type)
        return input_memories
        
    def create_pairs(self, duplicate=False) -> np.ndarray:
        """
        è¨˜æ†¶è¾æ›¸ã«ç™»éŒ²ã™ã‚‹ãŸã‚ã®ãƒšã‚¢ã‚’ä½œæˆã—ã¾ã™ã€‚
        duplicateãŒTrueã®æ™‚ã¯è¤‡æ•°ã®è¨˜æ†¶IDãŒä¸€ã¤ã®è¨˜æ†¶ã«é›†ã¾ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
            M_1 -> M_3
            M_2 -> M_3
        
        Create pairs to connect using Memory Dictionary.
        If duplicate is True, multiple memory IDs may be 
        connnected into a single memory.
            M_1 -> M_3
            M_2 -> M_3
        """

        srcs = self.memories.copy()
        if duplicate:
            tgt_idx = np.random.randint(0,len(self.memories),len(self.memories))
            tgts = self.memories[tgt_idx]
        else:
            tgts = self.memories.copy()
            np.random.shuffle(tgts)
        
        pairs = np.stack([srcs,tgts]).T
        return pairs

    def load_memories(self, memories:np.ndarray) -> None:
        """
        Working Memoryã«è¨˜æ†¶ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
        loading memories to Working Memory.
        """
        assert len(memories)<= self.max_length
        dt = memories.dtype
        if self.id_type != dt:
            warning("loading memories dtype: {} is not self.id_type: {}! ".format(dt,self.id_type))
        self.memories = memories.astype(self.id_type)
        info("loaded memories")
