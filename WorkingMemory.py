"""
ä¸€æ™‚è¨˜æ†¶ã®å®Ÿè£…ã‚¯ãƒ©ã‚¹ã§ã™ã€‚
å®šç¾©
    ä¸€æ™‚çš„ã«è¨˜æ†¶ã‚’è¦ç´ ã¨ã—ã¦ãã‚Œã‚’ä¿æŒã—ã¦ãŠãè¦ç´ æ•° ğ›¼ ã®æœ‰é™é›†åˆã€‚ğ‘Šã®è¦ç´ æ•°ãŒ ğ›¼ ã®æ™‚ã«è¦ç´ ã‚’è¿½åŠ ã™ã‚‹å ´
    åˆã€ğ‘Š ã®ä¸­ã®è¦ç´ ã®ã©ã‚Œã‹ãŒãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã°ã‚Œä¸Šæ›¸ãã•ã‚Œã‚‹ã€‚è¦ç´ æ•°ğ‘˜ | ğ‘˜ â‰¤ ğ›¼ã®ğ‘Šã¨è¨˜æ†¶ã‚’è¦ç´ ã¨ã™ã‚‹è¦ç´ æ•°
    ğ‘™ | ğ‘™ â‰¤ ğ›¼ å€‹ã®é›†åˆã¨çµåˆã™ã‚‹å ´åˆã€ğ‘Š ã®ä¸­ã‹ã‚‰ ğ‘˜ + ğ‘™ âˆ’ ğ›¼ å€‹ã®è¦ç´ ãŒãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã°ã‚Œä¸Šæ›¸ãã•ã‚Œã‚‹ã€‚

The Implementation class of "Working Memory".
*Definition
    Working Memory is a finite set with a maximum number of elements Î± that holds memories temporarily as an
    element. If the elements are added when the number of elements in W is Î±, the elements in W are randomly chosen
    and overwritten
"""
import torch
import numpy as np
from typing import *
from logger import *

setLogger(__name__,DEBUG)

class WorkingMemory:
    """
    ã“ã®ã‚¯ãƒ©ã‚¹ã¯æ¬¡ã®ã‚ˆã†ã«ã€è¨˜æ†¶ã‚’æ ¼ç´ã—ã¦ãŠããŸã‚ã®ã‚¯ãƒ©ã‚¹ã§ã™ã€‚
        W = {M_1, M_2, M_3}
    """

    def __init__(self, max_length:int, id_type:np.dtype=np.int64) -> None:
        """
        ç©ºã®Working Memory ã‚’ä½œæˆã—ã¾ã™
        Create a empty Working Memory.
        """

        self.id_type = id_type
        type_info = np.iinfo(id_type)
        self.min_id = 0
        self.max_id = type_info.max
        self.max_length = max_length

        self.memories = np.empty((0,),dtype=id_type)

    def add(self, input_memories:Union[int,list[int], np.ndarray,torch.Tensor], 
            is_sorted=True, is_duplicated=False) -> None:
        """
        Working Memoryã«è¨˜æ†¶ã‚’è¿½åŠ ã—ã¾ã™ã€‚
        ã™ã§ã«Working Memory ãŒã„ã£ã±ã„ã®ã¨ãã¯ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã°ã‚Œã€ä¸Šæ›¸ãã•ã‚Œã¾ã™ã€‚
        input_memoriesãŒæ˜‡é †ã«sortã„ãªã„å ´åˆã¯is_sorted=Falseã«ã—ã¦ãã ã•ã„ã€‚
        input_memoriesã®ä¸­ã«ã«é‡è¤‡ã—ãŸè¨˜æ†¶IDãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯is_duplicated=Trueã«ã—ã¦ãã ã•ã„

        Add input_memories to Working Memory.
        If the elements are added when the number of elements in W is maximum, 
        the elements in W are randomly chosen and overwritten.
        If input_memories is not sorted in ascending order, please set is_sorted=False
        If there are duplicate memory IDs in input_memories, please set is_duplicated=True
        """
        
        input_memories = self._type_check(input_memories)

        # fix duplication and sort.
        if is_duplicated:
            input_memories = np.unique(input_memories)
        elif not is_sorted:
            input_memories = np.sort(input_memories)

        # concatenation
        idxes = np.searchsorted(input_memories,self.memories)
        n_exist = input_memories[idxes] != self.memories
        mem = self.memories[n_exist]
        np.random.shuffle(mem)
        cut_len = self.max_length - len(input_memories)
        self.memories = np.concatenate([input_memories,mem[:cut_len]])

    def _type_check(self,input_memories) -> np.ndarray:
        t = type(input_memories)
        if t is list:
            input_memories = np.array(input_memories,self.id_type)
        elif t is int:
            input_memories = np.array([input_memories],self.id_type)
        elif t is torch.Tensor:
            input_memories = input_memories.detach().cpu().numpy()
        elif t is np.ndarray:
            pass
        else:
            raise ValueError("Unknow id data type! {}".format(t))
        
        input_memories = input_memories.astype(self.id_type)
        return input_memories
        
    def create_pairs(self, duplicate=False) -> np.ndarray:
        """
        è¨˜æ†¶è¾æ›¸ã«ç™»éŒ²ã™ã‚‹ãŸã‚ã®ãƒšã‚¢ã‚’ä½œæˆã—ã¾ã™ã€‚
        duplicateãŒTrueã®æ™‚ã¯è¤‡æ•°ã®è¨˜æ†¶IDãŒä¸€ã¤ã®è¨˜æ†¶ã«é›†ã¾ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
            M_1 -> M_3
            M_2 -> M_3
        
        Create pairs to connect using Memory Dictionary.
        If duplicate is True, multiple memory IDs may be 
        connnected into a single memory.
            M_1 -> M_3
            M_2 -> M_3
        """

        srcs = self.memories.copy()
        if duplicate:
            tgt_idx = np.random.randint(0,len(self.memories),len(self.memories))
            tgts = self.memories[tgt_idx]
        else:
            tgts = self.memories.copy()
            np.random.shuffle(tgts)
        
        pairs = np.stack([srcs,tgts]).T
        return pairs

    def load_memories(self, memories:np.ndarray) -> None:
        """
        Working Memoryã«è¨˜æ†¶ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
        loading memories to Working Memory.
        """
        assert len(memories)<= self.max_length
        dt = memories.dtype
        if self.id_type != dt:
            warning("loading memories dtype: {} is not self.id_type: {}! ".format(dt,self.id_type))
        self.memories = memories.astype(self.id_type)
        info("loaded memories")

    def __str__(self):
        return f"Working Memory\nid_type: {self.id_type}\nmax_length: {self.max_length}\ncontaining: {self.memories}"

    def clear(self):
        self.memories = np.empty((0,),dtype=self.id_type)


def test():
    id_type = np.longlong
    wm = WorkingMemory(8,id_type)
    debug(wm)
    info("contructor OK")
    
    # adding check
    addes = [
        [0,1,2,3,4],
        6,
        np.arange(7,10),
        torch.arange(10,15),
    ]
    for i, m in enumerate(addes):
        wm.add(m)
        debug(f"add {m}, memories are {wm.memories}")
    info("normal type addtion OK")

    un_sorted = [16,15,19,17,18]
    wm.add(un_sorted,is_sorted=False)
    debug(f"add unsorted {un_sorted}, memories are {wm.memories}")
    
    dups = [20,20,21,22,23]
    wm.add(dups,is_duplicated=True)
    debug(f"add duplication {dups}, memories are {wm.memories}")
    
    # pairs check
    pairs= wm.create_pairs()
    debug("pairs are {}".format(pairs))
    pairs = wm.create_pairs(True)
    debug("duplicated pairs are {}".format(pairs))
    info("Create pair OK")

    # load memories check
    memories = np.arange(10,dtype=id_type)
    try:
        wm.load_memories(memories)
        error("assertion in load_memories is not working")
    except AssertionError:
        info("assertion in load_memories is working")
    memories = np.arange(5,dtype=id_type)
    wm.load_memories(memories)
    if not (wm.memories == memories).any():
        error("loading memories is faild. wm.memories: {}, memories: {}".format(wm.memories,memories))
    else:
        info("load_memories OK")
    warning("Did a warning appear below? â†“")
    wm.load_memories(memories.astype(int))


if __name__ == "__main__":
    test()


